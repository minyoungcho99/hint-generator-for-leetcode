{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNE_a7JgXGxq",
        "outputId": "28a4d362-22c5-4428-82ca-1dbff329af4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from tokenize import tokenize\n",
        "from io import BytesIO\n",
        "import re"
      ],
      "metadata": {
        "id": "udr9kRKbYRay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#converting the raw data for CodeXGLUE#"
      ],
      "metadata": {
        "id": "OSC0zcRG3MMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_path = '/content/drive/MyDrive/CS4650_proj/data/leetcodecomplete.jsonl'\n",
        "output_dir = '/content/drive/MyDrive/CS4650_proj/data/clean_data.jsonl'"
      ],
      "metadata": {
        "id": "4IrvU9ZNHytI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_code(code):\n",
        "    tokens = []\n",
        "    for tok in tokenize(BytesIO(code.encode('utf-8')).readline):\n",
        "        if tok.type == 1 or tok.type == 54:  # Name or operator\n",
        "            tokens.append(tok.string)\n",
        "        elif tok.type == 2 or tok.type == 3 or tok.type == 51:  # Number, string, or comment\n",
        "            tokens.append(tok.string)\n",
        "    return tokens\n",
        "\n",
        "def tokenize_text(text):\n",
        "    # Removing special characters and tokenizing by spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "# Updated function to parse JSONL file and extract Python code snippets along with input as docstrings\n",
        "def parse_jsonl_for_python_code_with_docstrings(file_path):\n",
        "    entries = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for i, line in enumerate(file):\n",
        "            entry = json.loads(line)\n",
        "            if 'output' in entry and 'input' in entry:\n",
        "                python_code = entry['output'].strip('```python\\n').strip('```').strip()\n",
        "                code_tokens = tokenize_code(python_code)\n",
        "                input_text = entry['input']\n",
        "                docstring_tokens = tokenize_text(input_text)\n",
        "                entries.append({\n",
        "                    \"code_tokens\": code_tokens,\n",
        "                    \"docstring_tokens\": docstring_tokens\n",
        "                })\n",
        "    return entries\n",
        "\n",
        "def parse_raw_input_step(file_path):\n",
        "    dataset1 = []  # Dataset for segments with ' 2 ' or 'then', and other full inputs\n",
        "    dataset2 = []  # Dataset for the rest of the segments if input is split\n",
        "    delimiter_pattern = re.compile(r' \\b2\\b |then', flags=re.IGNORECASE)\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            entry = json.loads(line.strip())\n",
        "            if 'output' in entry and 'input' in entry:\n",
        "                python_code = entry['output'].strip('```python\\n').strip('```').strip()\n",
        "                code_tokens = tokenize_text(python_code)\n",
        "                input_text = entry['input']\n",
        "\n",
        "                parts = delimiter_pattern.split(input_text)\n",
        "                matches = delimiter_pattern.findall(input_text)\n",
        "\n",
        "                # Tokenize each part and add it to the appropriate dataset\n",
        "                for i in range(len(parts)):\n",
        "                    part_tokens = tokenize_text(parts[i].strip())\n",
        "                    if i < len(matches):  # For parts before a delimiter\n",
        "                        dataset1.append({\n",
        "                            \"code_tokens\": code_tokens,\n",
        "                            \"docstring_tokens\": part_tokens\n",
        "                        })\n",
        "                    else:  # For the last part or if no matches\n",
        "                        if matches:\n",
        "                            dataset2.append({\n",
        "                                \"code_tokens\": code_tokens,\n",
        "                                \"docstring_tokens\": part_tokens\n",
        "                            })\n",
        "                        else:\n",
        "                            dataset1.append({\n",
        "                                \"code_tokens\": code_tokens,\n",
        "                                \"docstring_tokens\": part_tokens\n",
        "                            })\n",
        "\n",
        "    return dataset1, dataset2\n",
        "\n",
        "\n",
        "def save_to_jsonl(data, output_file_path):\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "        for item in data:\n",
        "            json_line = json.dumps(item)\n",
        "            file.write(json_line + '\\n')"
      ],
      "metadata": {
        "id": "ZKqyDW4ca85K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = parse_jsonl_for_python_code_with_docstrings(jsonl_path)\n",
        "train, temp = train_test_split(results, test_size=0.3, random_state=42)\n",
        "test, val = train_test_split(temp, test_size=1/3, random_state=42)\n",
        "\n",
        "train_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_train.jsonl'\n",
        "test_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_test.jsonl'\n",
        "val_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_val.jsonl'\n",
        "\n",
        "save_to_jsonl(train, train_path)\n",
        "save_to_jsonl(test, test_path)\n",
        "save_to_jsonl(val, val_path)"
      ],
      "metadata": {
        "id": "NRwx1er2CHIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_step1, results_rest = parse_raw_input_step(jsonl_path)\n",
        "\n",
        "train_step1, temp_step1 = train_test_split(results_step1, test_size=0.3, random_state=42)\n",
        "test_step1, val_step1 = train_test_split(temp_step1, test_size=1/3, random_state=42)\n",
        "\n",
        "train_rest, temp_rest = train_test_split(results_step1, test_size=0.3, random_state=42)\n",
        "test_rest, val_rest = train_test_split(temp_rest, test_size=1/3, random_state=42)\n",
        "\n",
        "train_step1_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_train_step1.jsonl'\n",
        "test_step1_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_test_step1.jsonl'\n",
        "val_step1_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_val_step1.jsonl'\n",
        "\n",
        "train_rest_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_train_rest.jsonl'\n",
        "test_rest_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_test_rest.jsonl'\n",
        "val_rest_path = '/content/drive/MyDrive/CS4650_proj/data/clean_data_val_rest.jsonl'\n",
        "\n",
        "save_to_jsonl(train_step1, train_step1_path)\n",
        "save_to_jsonl(test_step1, test_step1_path)\n",
        "save_to_jsonl(val_step1, val_step1_path)\n",
        "\n",
        "save_to_jsonl(train_rest, train_rest_path)\n",
        "save_to_jsonl(test_rest, test_rest_path)\n",
        "save_to_jsonl(val_rest, val_rest_path)"
      ],
      "metadata": {
        "id": "isNn7T3BVax-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}